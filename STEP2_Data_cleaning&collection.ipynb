{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e410363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eadb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "battery_names_file = open(\"battery_names_file.pkl\", \"rb\")\n",
    "bat_dict = pickle.load(battery_names_file)\n",
    "\n",
    "num_cycles = np.loadtxt('num_cycles.csv', dtype=int)\n",
    "\n",
    "test_ind = np.loadtxt('test_ind.csv', dtype=int, delimiter=',')\n",
    "train_ind = np.loadtxt('train_ind.csv', dtype=int, delimiter=',')\n",
    "secondary_test_ind = np.loadtxt('secondary_test_ind.csv', dtype=int, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "61f63782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_c = {}\n",
    "# keys = range(124)\n",
    "# for i in keys:\n",
    "#     dict_c[i] = num_cycles[i]\n",
    "\n",
    "# for key, value in dict_c.items():\n",
    "#     print(key, ' : \\n', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258bbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_all_batt_all_c = []\n",
    "\n",
    "for batt in range(0, len(bat_dict.keys())):\n",
    "    \n",
    "    #if batt ==63:\n",
    "    #    continue     \n",
    "        \n",
    "    features_data = pickle.load(open(r'./F_tot_batt{}.pkl'.format(batt),'rb'))\n",
    "    \n",
    "    cycle_num = len(features_data)    \n",
    "    features_size = len(features_data[0][0])\n",
    "    count = 0\n",
    "    F_1c = np.zeros((1, features_size))\n",
    "    F_allc = np.zeros((1, features_size))   #fictious value\n",
    "    Q_allc = []\n",
    "    \n",
    "    #if batt == 42 or batt ==63 or batt ==65:\n",
    "\n",
    "    for c in range(0, cycle_num):\n",
    "    #for c in range(0, 200):\n",
    "        \n",
    "        if len(features_data[c][0][:, 1])<68:\n",
    "            F_1c[0, :24] = features_data[c][0][:24, 1]\n",
    "            F_1c[0, -24:] = features_data[c][0][-24:, 1]\n",
    "            F_1c[0, 24:-24] = np.linspace(features_data[c][0][23, 1], features_data[c][0][-24, 1], 20) \n",
    "            count+=1\n",
    "        else:\n",
    "            F_1c = features_data[c][0][:, 1].reshape(1, -1)\n",
    "\n",
    "        Q_1c = features_data[c][1]\n",
    "        t_max_1c = features_data[c][2]\n",
    "        \n",
    "        F_allc = np.r_[F_allc, F_1c]\n",
    "        Q_allc.append(Q_1c)\n",
    "\n",
    "        #F is a list \n",
    "        #column 0 contains cycle# arrays of 68 features (call F_tot[0][cycle])\n",
    "        #2nd column contains Q F_tot[1][cycle]\n",
    "    \n",
    "    F_tot = [F_allc[1:, :], Q_allc]       #remove first row of F_all_c that was fictious\n",
    "    F_all_batt_all_c.append(F_tot)\n",
    "    print('batt #', batt)\n",
    "    print('# exceptions:', count)  # count cases in which #Features<68\n",
    "    \n",
    "with open(\"F_all_batt_all_c\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(F_all_batt_all_c, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8191ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove outliers from F_all_batt_all_c \n",
    "## the criteria adopted is take windows of 10 points and use hampel filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2cf24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"F_all_batt_all_c\", \"rb\") as fp:   # Unpickling\n",
    "    F_all_batt_all_c = pickle.load(fp)\n",
    "    \n",
    "F_all_batt_clean = F_all_batt_all_c.copy()\n",
    "for idx in range(0, len(F_all_batt_clean)):\n",
    "#for idx in range(8, 9):\n",
    "    V_feature_tot = F_all_batt_all_c[idx][0].copy()\n",
    "    Q_feature_tot = np.array(F_all_batt_all_c[idx][1]).reshape(-1, 1).copy()\n",
    "    batch_i_tot = np.c_[Q_feature_tot, V_feature_tot]   # copy capacities in position 1 and voltage features in position 2\n",
    "\n",
    "    F_old = batch_i_tot.copy()    \n",
    "    \n",
    "    window_size= 10   \n",
    "    ## this modify the tail in features where there is a spike at the end of curve\n",
    "    \n",
    "    for kk in range(0, batch_i_tot.shape[1]):\n",
    "        df = pd.Series(batch_i_tot[:, kk])\n",
    "        pos = hampel(df, window_size, n=1) \n",
    "        windows = df.rolling(window_size, min_periods=1)\n",
    "        moving_averages = windows.median().tolist()\n",
    "        for ii in range(0, len(pos)):\n",
    "            batch_i_tot[pos[ii], kk] = np.array(moving_averages[pos[ii]])\n",
    "    \n",
    "    for kk in range(0, batch_i_tot.shape[1]):\n",
    "        plt.figure()\n",
    "        plt.plot(np.linspace(0, 1, len(F_old)), F_old[:, kk], 'k.')\n",
    "        plt.plot(np.linspace(0, 1, len(batch_i_tot)), batch_i_tot[:, kk], 'r.')\n",
    "        #plt.ylim(batch_i_tot[:, kk].min(), batch_i_tot[:, kk].max())\n",
    "        plt.title('batt{}_F{}'.format(idx, kk))\n",
    "\n",
    "    F_all_batt_clean[idx][0] = batch_i_tot[:, 1:-1]\n",
    "    F_all_batt_clean[idx][1] = batch_i_tot[:, 0]\n",
    "    \n",
    "    for kk in range(0, batch_i_tot.shape[1]):\n",
    "        plt.figure()\n",
    "        plt.plot(np.linspace(0, 1, len(F_old)), F_old[:, kk], 'b.')\n",
    "        plt.plot(np.linspace(0, 1, len(batch_i_tot)), batch_i_tot[:, kk], 'g.')\n",
    "        #plt.ylim(batch_i_tot[:, kk].min(), batch_i_tot[:, kk].max())\n",
    "        plt.title('batt{}_F{}'.format(idx, kk))\n",
    "        \n",
    "with open(\"F_all_batt_clean\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(F_all_batt_clean, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### see how the features are with different batteries in train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16bbc9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard batteries 41, 58, 59\n",
    "# open num_cycleswIv4\n",
    "\n",
    "print(train_ind, 'train_ind')\n",
    "print(test_ind, 'test_ind')\n",
    "print(secondary_test_ind, 'secondary_test_ind')\n",
    "#train_ind = np.delete(train_ind, np.where(train_ind==41)[0])\n",
    "#train_ind = np.delete(train_ind, np.where(train_ind==57)[0])    ## this is wrong.. batt 59 correspond to new idx 57 in F_all_batt_clean_wI4\n",
    "#test_ind = np.delete(test_ind, np.where(test_ind==58)[0])  # batt 58 correspond to idx 57 in F_all_batt_clean_wI4\n",
    "print(train_ind, 'train_ind')\n",
    "print(test_ind, 'test_ind')\n",
    "secondary_test_ind = np.delete(secondary_test_ind, -1)\n",
    "secondary_test_ind = np.delete(secondary_test_ind, -1)\n",
    "secondary_test_ind = np.delete(secondary_test_ind, -1)\n",
    "secondary_test_ind = np.delete(secondary_test_ind, -1)\n",
    "print(secondary_test_ind, 'secondary_test_ind')\n",
    "\n",
    "\n",
    "num_cycles = np.delete(num_cycles, 41)\n",
    "num_cycles = np.delete(num_cycles, 57)\n",
    "num_cycles = np.delete(num_cycles, 57)\n",
    "\n",
    "\n",
    "## collect C rates\n",
    "dt = np.dtype(object)\n",
    "charge_policy_list = np.loadtxt('./charge_policy_dict.csv', dtype=dt, delimiter=',', skiprows=1)\n",
    "charge_policy_list = np.delete(charge_policy_list, 41)   \n",
    "charge_policy_list = np.delete(charge_policy_list, 57) \n",
    "charge_policy_list = np.delete(charge_policy_list, 57)\n",
    "\n",
    "C1_list = []\n",
    "C2_list = []\n",
    "P1_list = []\n",
    "\n",
    "for idx in range(0, len(charge_policy_list)):\n",
    "  #C_rate = bat_dict[idx]['charge_policy']\n",
    "    C_rate_i = charge_policy_list[idx]\n",
    "    print(C_rate_i)\n",
    "    C_rate_list = re.findall(r'-?\\d+\\.?\\d*', C_rate_i)\n",
    "        \n",
    "    C1 = np.fromstring(C_rate_list[0], dtype='float64', sep=\" \")\n",
    "    P1 = np.fromstring(C_rate_list[1], dtype='float64', sep=\" \")\n",
    "    C2 = -np.fromstring(C_rate_list[2], dtype='float64', sep=\" \")\n",
    "\n",
    "    C1_list.append(C1)\n",
    "    C2_list.append(C2)\n",
    "    P1_list.append(P1)\n",
    "\n",
    "print(' C1 max is:', np.array(C1_list).max())\n",
    "print(' C1 min is:', np.array(C1_list).min())\n",
    "print(' C2 max is:', np.array(C2_list).max())\n",
    "print(' C2 min is:', np.array(C2_list).min())\n",
    "print(' P1 max is:', np.array(P1_list).max())\n",
    "print(' P1 min is:', np.array(P1_list).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7910ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./../../data/F_all_batt_clean\", \"rb\") as fp:   # Unpickling\n",
    "    F_all_batt_all_c = pickle.load(fp)\n",
    "\n",
    "list_element_key = lambda x: list(bat_dict)[x]\n",
    "train_key_list = [list_element_key(x) for x in(train_ind)]\n",
    "valid_key_list = [list_element_key(x) for x in(test_ind)]\n",
    "train2_key_list = [list_element_key(x) for x in(secondary_test_ind)]\n",
    "\n",
    "N_train = len(train_key_list)\n",
    "print(N_train, 'N_train')\n",
    "N_test = len(valid_key_list)\n",
    "print(N_test, 'N_test')\n",
    "N_train_2 = len(train2_key_list)\n",
    "print(N_train_2, 'N_train_2')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d080052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_name(dict):\n",
    "    opsep = '_'\n",
    "    name = 'LSTM20p2'\n",
    "    for key, value in dict.items():\n",
    "        name += '{}'.format(key)+opsep+'{}'.format(value)+opsep\n",
    "    return name\n",
    "\n",
    "\n",
    "class LogDataset_():\n",
    "    def __init__(self, idx_list, F_all_batt, sampling_freq, Crate1, Crate2, Prate, load_mode): \n",
    "        self.idx_list = idx_list\n",
    "        self.load_mode = load_mode\n",
    "        self.F_all_batt = F_all_batt\n",
    "        self.Crate1 = Crate1\n",
    "        self.Crate2 = Crate2\n",
    "        self.Prate = Prate\n",
    "               \n",
    "    def __len__(self):\n",
    "        return len(self.idx_list)\n",
    "          \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        V_feature_tot = np.delete(self.F_all_batt[self.idx_list[idx]][0], [0, 25, 37, 44, 45, -1], 1)\n",
    "        Q_feature_tot = np.array(self.F_all_batt[self.idx_list[idx]][1]).reshape(-1, 1)\n",
    "        lendata = tmax_feature_tot.shape[0]\n",
    "\n",
    "        batch_i_tot = np.c_[Q_feature_tot, V_feature_tot]\n",
    "        \n",
    "        #here we normalize the features\n",
    "        batch_i_min = batch_i_tot.min(axis = 0).reshape(1, -1)\n",
    "        batch_i_max = batch_i_tot.max(axis = 0).reshape(1, -1)\n",
    "        batch_i_norm = (batch_i_tot-batch_i_min)/(batch_i_max-batch_i_min)\n",
    "        C1_norm = (self.Crate1[self.idx_list[idx]]-1)/7\n",
    "        C2_norm = (self.Crate2[self.idx_list[idx]]-3)/3\n",
    "        P_norm = (self.Prate[self.idx_list[idx]]-2)/78\n",
    "\n",
    "        #here we add the charge policy into the set of features\n",
    "        batch_i_n = np.c_[batch_i_norm, C1_norm.repeat(lendata).reshape(-1, 1), C2_norm.repeat(lendata).reshape(-1, 1), P_norm.repeat(lendata).reshape(-1, 1)]\n",
    "        #here we select only the features from feature sub-sampling\n",
    "        select_features = [0, 1, 2, 3, 5, 6, 8, 10, 12, 14, 20, 22, 23, 24, 26, 28, 29, 30, 36, 37, 41, 64, 65, 66]      #CM Re Final\n",
    "        select_features = [ 0,  4,  5,  9, 11, 12, 14, 21, 22, 25, 30, 33, 35, 36, 37, 39, 44, 48, 55, 56, 58, 59, 64, 66]   # Sal Re Final\n",
    "\n",
    "        batch_i_ss = batch_i_n[::(sampling_freq//1), :]      #with sampling frequency !=1 we can select the frequency of data points along cycles \n",
    "        \n",
    "        print(batch_i_ss.shape)\n",
    "         \n",
    "        if self.load_mode == 'all':\n",
    "            batch_i_s = batch_i_ss[::1, :]\n",
    "            true_y = torch.zeros(2973//sampling_freq, batch_i_s.shape[1], dtype=torch.float32)                               #e\n",
    "            batch_i_s_torch = torch.from_numpy(batch_i_s).type(torch.float32)\n",
    "            if len(batch_i_s_torch)==2973//sampling_freq+1:\n",
    "                true_y[:len(batch_i_s_torch)-1, :] = batch_i_s_torch[:-1, :]\n",
    "                batch_y = torch.unsqueeze(true_y, 0)    # (#batt, #steps, #features)\n",
    "                pad = len(batch_i_s)-1\n",
    "            else:\n",
    "                true_y[:len(batch_i_s_torch), :] = batch_i_s_torch\n",
    "                batch_y = torch.unsqueeze(true_y, 0)    # (#batt, #steps, #features)\n",
    "                pad = len(batch_i_s)\n",
    "                   \n",
    "        return batch_y, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "\n",
    "def makedirs(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "                    \n",
    "os.system('clear')\n",
    "\n",
    "sampling_freq = 1\n",
    "feature_size = 67 #67, #24, #25, #64\n",
    "\n",
    "list_element_key = lambda x: list(bat_dict)[x]\n",
    "train_key_list = [list_element_key(x) for x in(train_ind)]\n",
    "valid_key_list = [list_element_key(x) for x in(test_ind)]\n",
    "test_key_list = [list_element_key(x) for x in(secondary_test_ind)]\n",
    "\n",
    "N_train = len(train_ind)+N_train_2\n",
    "\n",
    "data_size = 2973//sampling_freq\n",
    "\n",
    "## Set parameters\n",
    "# Data params\n",
    "batch_size = data_size\n",
    "test_size = data_size\n",
    "\n",
    "# Network params\n",
    "input_size = feature_size\n",
    "output_dim = feature_size                      \n",
    "dtype = torch.float32\n",
    "\n",
    "N_train = len(train_ind)+N_train_2\n",
    "N_valid = int(0.1*len(test_ind))\n",
    "N_test = len(test_ind)-N_valid\n",
    "\n",
    "valid_idx, test_idx = random_split(test_ind, [N_valid, N_test], generator=torch.Generator().manual_seed(1))\n",
    "a = DataLoader(valid_idx, len(valid_idx), shuffle=False)\n",
    "b = DataLoader(test_idx, len(test_idx), shuffle=False)\n",
    "for i, idx in enumerate(a):\n",
    "    v_indx = idx.detach().numpy()\n",
    "for i, idx in enumerate(b):\n",
    "    t_indx = idx.detach().numpy()\n",
    "\n",
    "all_ind = np.append(train_ind, secondary_test_ind)  \n",
    "all_ind = np.append(all_ind, v_indx)\n",
    "all_ind = np.append(all_ind, t_indx)\n",
    "\n",
    "all_ind_rep = np.append(all_ind[:85].repeat(2), all_ind[85:])    #here we repeat the training batteries twice to train more data\n",
    "train_all = LogDataset_(all_ind_rep, F_all_batt_all_c, sampling_freq, C1_list, C2_list, P1_list, load_mode ='all')\n",
    "dataloader_all  = DataLoader(train_all, batch_size=len(train_all), shuffle=False)\n",
    "\n",
    "# prepare the data\n",
    "for i, n in enumerate(dataloader_all):\n",
    "    train_all_input = n[0].squeeze(1)[:, :, :]\n",
    "    train_all_target = n[0].squeeze(1)[:, 1:, :]\n",
    "    L_all = n[1]+0.33*n[1]   #we add a plateau at the end of each dataset to allow longer predictions\n",
    "\n",
    "L_all = torch.from_numpy(np.ceil(np.array(L_all))).type(torch.int32)\n",
    "\n",
    "print(L_all, 'L_all')\n",
    "print(len(L_all), 'len stacked data')\n",
    "\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "N_interp = 100 \n",
    "N_b = len(L_all)\n",
    "\n",
    "train_interp = train_all_input.clone() \n",
    "N_b, N_max_step, N_F = train_interp.shape \n",
    "train_interp_w_time = torch.empty((N_b, N_interp, N_F+1), dtype = torch.float32) \n",
    "target_interp_w_time = torch.empty((N_b, N_interp, N_F+1), dtype = torch.float32)\n",
    "\n",
    "n_max = 0.3 # value to optimize \n",
    "n_pred = 4\n",
    "\n",
    "random_portions1 = np.random.uniform(low=0.1, high=n_max, size=(85+85,)) # range of interpolation of training data \n",
    "random_portions2 = np.random.uniform(low=0.1, high=0.3, size=(12,)) \n",
    "random_portions3 = np.random.uniform(low=0.2, high=0.2, size=(24,))\n",
    "random_portions = np.concatenate((random_portions1, random_portions2, random_portions3)) \n",
    "print(random_portions, 'random_portions')\n",
    "\n",
    "train_interp_pred = torch.empty((N_b, int(np.floor(N_interp*n_pred)), N_F+1), dtype = torch.float32) \n",
    "target_interp_pred = torch.empty((N_b, int(np.floor(N_interp*n_pred)), N_F+1), dtype = torch.float32)\n",
    "\n",
    "for ii in range(0, N_b):\n",
    "\n",
    "    L_ii = int(L_all[ii])\n",
    "    L_partial = np.floor(random_portions[ii]*L_ii/1.33) \n",
    "    L_pred = np.ceil(L_partial*n_pred)\n",
    "\n",
    "    print(L_ii, 'L_ii')\n",
    "    print(L_partial, 'L_partial')\n",
    "    print(random_portions[ii], 'random_portions[ii]')\n",
    "    \n",
    "    max_L = 2234\n",
    "    \n",
    "    delta = int(L_partial)/N_interp\n",
    "    t_N_tilde = torch.arange(0, int(L_partial), delta, dtype = torch.float32)/max_L\n",
    "    t_tilde_RUL = torch.arange(0, L_pred, delta, dtype = torch.float32)/max_L\n",
    "\n",
    "    train_interp_w_time[ii, :, 0] = t_N_tilde[:N_interp]\n",
    "    target_interp_w_time[ii, :, 0] = train_interp_w_time[ii, :, 0]+1/max_L\n",
    "\n",
    "    train_interp_pred[ii, :, 0] = t_tilde_RUL[:int(np.floor(N_interp*n_pred))].type(torch.float32)\n",
    "    target_interp_pred[ii, :, 0] = train_interp_pred[ii, :int(np.floor(N_interp*n_pred)), 0]+1/2234\n",
    "\n",
    "    t_N_tilde_target = target_interp_w_time[ii, :N_interp, 0].type(torch.float32).tolist()\n",
    "    t_tilde_RUL_target = target_interp_pred[ii, :int(np.floor(N_interp*n_pred)), 0].type(torch.float32).tolist()\n",
    "\n",
    "    for k in range(0, N_F):\n",
    "        if L_ii<= 2972:\n",
    "            y_3 = train_interp[ii, :L_ii+1, k].reshape(-1)   ## this is the vector containing all data\n",
    "            x_3 = torch.arange(0, L_ii+1, 1)/max_L                  ## vector of cycles until EoL\n",
    "        elif L_ii> 2972:\n",
    "            y_3 = train_interp[ii, :L_ii, k].reshape(-1)   ## this is the vector containing all data\n",
    "            x_3 = torch.arange(0, L_ii)/max_L         \n",
    "        else:\n",
    "            y_3 = train_interp[ii, :L_ii+1, k].reshape(-1)   ## this is the vector containing all data\n",
    "            x_3 = torch.arange(0, L_ii+1, 1)/max_L                  ## vector of cycles until EoL\n",
    "            \n",
    "        #print(t_N_tilde_target, 't_N_tilde')\n",
    "        #print(t_tilde_RUL_target, 't_tilde_RUL')\n",
    "\n",
    "        interpolate_func = interpolate.interp1d(x_3, y_3, kind='cubic')\n",
    "\n",
    "        ## derive the interpolated function (train, target) until random portion\n",
    "        qinterp_train = (interpolate_func(t_N_tilde[:N_interp])).reshape(-1)\n",
    "        train_interp_w_time[ii, :, k+1] = torch.from_numpy(qinterp_train.astype(np.float32))       \n",
    "\n",
    "        qinterp_target = (interpolate_func(t_N_tilde_target)).reshape(-1)\n",
    "        target_interp_w_time[ii, :, k+1] = torch.from_numpy(qinterp_target.astype(np.float32))   \n",
    "\n",
    "        ## derive the interpolated function (train, target) for forecast, from random portion on\n",
    "\n",
    "        qinterp_train_pred = (interpolate_func(t_tilde_RUL[:int(np.floor(N_interp*n_pred))])).reshape(-1)\n",
    "        train_interp_pred[ii, :, k+1] = torch.from_numpy(qinterp_train_pred.astype(np.float32))          \n",
    "\n",
    "        qinterp_target_pred = (interpolate_func(t_tilde_RUL_target[:int(np.floor(N_interp*n_pred))])).reshape(-1)\n",
    "        target_interp_pred[ii, :, k+1] = torch.from_numpy(qinterp_target_pred.astype(np.float32))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
